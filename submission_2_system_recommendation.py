# -*- coding: utf-8 -*-
"""Submission 2 System Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/157X5R_8JWWpSRuE9ws2W9u3h1B5KCa3U

# Book Recommendation Using Collaborative Filtering

# Import Library
"""

from google.colab import files
import os
import zipfile

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint

"""# Data Understanding

Dataset yang digunakan dalam proyek ini berasal dari Kaggle dengan judul [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data).

Berdasarkan informasi dari Kaggle, dataset ini terdiri dari tiga file utama yang saling terkait, yaitu:

1. Users (BX-Users.csv)

  Berisi informasi tentang pengguna.
  - Kolom User-ID adalah ID unik pengguna yang telah dianonimkan (dalam bentuk angka).
  - Kolom Location dan Age menyediakan data demografis jika tersedia. Jika tidak tersedia, nilainya berupa NULL.

2. Books (BX-Books.csv)
  
  Menyimpan informasi detail tentang buku yang tersedia.
  - Buku diidentifikasi berdasarkan ISBN yang valid.
  - Informasi tambahan mencakup: Book-Title, Book-Author, Year-Of-Publication, dan Publisher.
  - Dataset ini juga menyediakan URL gambar sampul buku dalam tiga ukuran: kecil (Image-URL-S), sedang (Image-URL-M), dan besar (Image-URL-L). URL ini mengarah ke situs Amazon.
  - Jika sebuah buku memiliki lebih dari satu penulis, hanya penulis pertama yang dicantumkan.

3. Ratings (BX-Book-Ratings.csv)

  Berisi data rating buku oleh pengguna.
  - Rating berupa angka pada skala 1–10 yang menunjukkan tingkat apresiasi (semakin tinggi semakin disukai).
  - Rating 0 dianggap sebagai interaksi implisit (misalnya user melihat atau memiliki buku, tetapi tidak memberikan penilaian eksplisit).

## Data Loading
"""

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Unduh dataset dari Kaggle
!kaggle datasets download -d arashnic/book-recommendation-dataset

!unzip book-recommendation-dataset.zip

books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')

books

books.info()

"""- books memiliki 271.360 baris dan 8 kolom.
- Terdapat informasi terkait ISBN, judul buku, penulis, tahun terbit, penerbit, serta link gambar buku.
- Namun, terdapat sedikit missing values di kolom Book-Author, Publisher, dan Image-URL-L.


"""

ratings

ratings.info()

"""ratings memiliki 1.149.780 entri rating dari pengguna terhadap buku, yang merupakan data utama untuk membangun sistem rekomendasi.

"""

users

users.info()

"""- users memiliki 278.858 pengguna.
- Data mencakup User-ID, Location, dan Age, namun kolom Age memiliki banyak missing values
"""

print('Jumlah pengguna unik: ', len(users['User-ID'].unique()))
print('Jumlah buku unik: ', len(books['ISBN'].unique()))
print('Jumlah rating (interaksi) yang diberikan pengguna: ', len(ratings))
print('Jumlah pengguna yang memberikan rating: ', len(ratings['User-ID'].unique()))
print('Jumlah buku yang menerima rating: ', len(ratings['ISBN'].unique()))

"""#  Exploratory Data Analysis

## Distribusi Ratings
"""

plt.figure(figsize=(8,5))
sns.countplot(data=ratings, x='Book-Rating')
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Data rating = 0 perlu ditangani karena merupakan sinyal perilaku pengguna (interaksi implisit), tetapi bukan rating yang menunjukkan tingkat apresiasi.

## Top 10 Buku paling banyak dirating
"""

most_rated_books = ratings['ISBN'].value_counts().head(10).reset_index()
most_rated_books.columns = ['ISBN', 'rating_count']
most_rated_books = most_rated_books.merge(books[['ISBN', 'Book-Title']], on='ISBN')

plt.figure(figsize=(10,6))
sns.barplot(data=most_rated_books, y='Book-Title', x='rating_count')
plt.title('Top 10 Buku yang Paling Banyak Dirating')
plt.xlabel('Jumlah Rating')
plt.ylabel('Judul Buku')
plt.show()

"""## Distribusi Umur Pengguna"""

sns.histplot(users['Age'].dropna(), bins=30, kde=True)
plt.title('Distribusi Umur Pengguna')
plt.xlabel('Umur')
plt.ylabel('Jumlah Pengguna')
plt.show()

"""## 10 User Teraktif"""

top_users = ratings['User-ID'].value_counts().head(10)
top_users.plot(kind='bar')
plt.title('Top 10 Pengguna dengan Rating Terbanyak')
plt.xlabel('User-ID')
plt.ylabel('Jumlah Rating')
plt.show()

"""# Data Preprocessing

## Missing value
"""

# Cek missing value
print(books.isnull().sum())
print(users.isnull().sum())

"""1. Kolom `Book-Author`, `Publisher`, dan `Image-URL-L` memiliki jumlah nilai kosong yang sangat kecil (kurang dari 5 nilai dari total lebih dari 270.000 baris). Oleh karena itu, nilai kosong ini dapat:
  - Diisi dengan teks placeholder seperti `"Unknown"` atau
  - Dihapus barisnya tanpa signifikan memengaruhi data


2. Kolom `Age` memiliki lebih dari 110.000 missing value dari total sekitar 278.000 data. Namun, karena proyek ini menggunakan pendekatan **collaborative filtering**, informasi demografis seperti `Age` tidak diperlukan secara langsung dalam proses rekomendasi.

  **Langkah yang diambil:**  
Kolom `Age` akan **diabaikan** dalam tahap pemodelan, dan nilai kosongnya tidak akan memengaruhi hasil.

"""

# Tangani missing value pada books dataset dengan teks "Unknown"
books['Book-Author'].fillna('Unknown', inplace=True)
books['Publisher'].fillna('Unknown', inplace=True)
books['Image-URL-L'].fillna('Unknown', inplace=True)

print("Cek missing value setelah ditangani:")
print(books.isnull().sum())
print(users.isnull().sum())

"""## Filter Rating = 0

Menghapus baris data dengan Book-Rating == 0.

Menurut dokumentasi dataset Kaggle, Rating = 0 merepresentasikan interaksi implisit, yaitu ketika pengguna melihat, mengakses, atau memiliki buku, tetapi tidak memberikan penilaian eksplisit terhadap buku tersebut.

Artinya, rating 0 bukan berarti buku itu tidak disukai atau diberi nilai buruk, melainkan tidak ada rating yang jelas dari pengguna.

Jika kita tidak memfilter rating 0 saat melatih model:
- Model bisa belajar dari data yang tidak akurat, karena rating 0 bukanlah penilaian nyata (eksplisit) terhadap kualitas buku.
- Prediksi menjadi bias atau tidak stabil, karena model berusaha mempelajari "rating" yang sebenarnya tidak diberikan oleh user.
- Bisa menurunkan performa model dalam mempelajari preferensi pengguna dan menghasilkan rekomendasi yang relevan.
"""

print(f"Jumlah rating sebelum filter rating 0: {len(ratings)}")

# Filter data ratings sama dengan 0
ratings = ratings[ratings['Book-Rating'] > 0]
print(f"Jumlah rating setelah filter rating 0: {len(ratings)}")

"""## Filter Data untuk Mengurangi Sparsity
Dataset setelah filter data rating == 0 memiliki 433.671 entri rating. Namun, banyak pengguna dan buku hanya memiliki sedikit rating, yang dapat menyebabkan matriks interaksi menjadi sangat sparse (jarang terisi). Hal ini dapat menghambat performa model collaborative filtering karena model kesulitan menemukan pola dari data yang sangat jarang.

Untuk itu, dilakukan filtering dengan ketentuan berikut:

- Hanya menyertakan pengguna yang telah memberikan minimal 10 rating.

- Hanya menyertakan buku yang telah menerima minimal 10 rating.
"""

# Filter pengguna dengan ≥10 rating
active_users = ratings['User-ID'].value_counts()
ratings = ratings[ratings['User-ID'].isin(active_users[active_users >= 10].index)]

# Filter buku dengan ≥10 rating
popular_books = ratings['ISBN'].value_counts()
ratings = ratings[ratings['ISBN'].isin(popular_books[popular_books >= 10].index)]

ratings.info()

"""- **Jumlah data rating sebelum diproses:** Dataset awal memiliki 1.149.780 entri rating.

- **Setelah Filter Rating = 0:** Jumlah data berkurang menjadi **433.671** entri rating. Ini adalah rating eksplisit pada skala 1–10.

- **Setelah Filter Sparsity:** Jumlah data yang tersisa setelah hanya menyertakan pengguna dan buku dengan minimal 10 rating eksplisit adalah **74.907** data.

- Meskipun cukup banyak data yang dihapus dari total awal, filtering ini penting untuk:
  - Fokus hanya pada rating eksplisit yang bermakna untuk prediksi rating buku (menghilangkan noise dari interaksi implisit atau rating 0).
  - Mengurangi sparsity (kepadatan data) pada matriks interaksi dengan membuang pengguna dan buku yang interaksinya terlalu sedikit.
  - Memastikan bahwa model dilatih pada data dari pengguna dan buku yang memiliki cukup riwayat interaksi eksplisit.

- Data yang tersisa sebanyak 74.907 entri, meskipun jauh lebih sedikit dari awal, tetap mencukupi untuk membangun model *collaborative filtering* yang efektif dan representatif pada populasi pengguna dan buku yang lebih aktif dan populer.

# Data Preparation
Pada tahap ini, kita akan melakukan persiapan data agar dapat digunakan dalam model rekomendasi berbasis collaborative filtering. Langkah utama yang dilakukan adalah menyandikan (encode) fitur User-ID dan ISBN menjadi indeks integer, sehingga model dapat dengan mudah memproses data tersebut.

##  Proses Encoding
"""

# Mengubah User-ID menjadi list unik
user_ids = ratings['User-ID'].unique().tolist()
print('List User-ID (unfiltered):', user_ids[:5])

# Melakukan encoding User-ID > index
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('User-ID to encoded:', list(user_to_user_encoded.items())[:5])

# Melakukan decoding index > User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('Encoded index to User-ID:', list(user_encoded_to_user.items())[:5])

# Mengubah ISBN menjadi list unik
book_ids = ratings['ISBN'].unique().tolist()
print('List ISBN (unfiltered):', book_ids[:5])

# Melakukan encoding ISBN > index
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
print('ISBN to encoded:', list(book_to_book_encoded.items())[:5])

# Melakukan decoding index > ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}
print('Encoded index to ISBN:', list(book_encoded_to_book.items())[:5])

"""## Mapping Hasil Encoding ke DataFrame"""

# Tambahkan kolom encoded ke dataframe ratings
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)
ratings['book'] = ratings['ISBN'].map(book_to_book_encoded)

ratings

"""## Eksplorasi Setelah Encoding"""

# Jumlah user dan buku unik
num_users = len(user_to_user_encoded)
num_books = len(book_to_book_encoded)

# Ubah rating ke float
ratings['Book-Rating'] = ratings['Book-Rating'].astype(np.float32)

# Rentang rating
min_rating = ratings['Book-Rating'].min()
max_rating = ratings['Book-Rating'].max()

print(f'Number of Users: {num_users}')
print(f'Number of Books: {num_books}')
print(f'Min Rating: {min_rating}, Max Rating: {max_rating}')

"""## Membagi Data untuk Training dan Validasi

Sebelum membagi data, kita acak terlebih dahulu agar distribusi data menjadi lebih merata dan tidak terurut berdasarkan user atau buku tertentu.
Langkah-langkah:
1. Acak dataframe
2. Gabungkan kolom user dan book sebagai input x
3. Normalisasi kolom Book-Rating ke skala 0–1 sebagai target y
4. Bagi data menjadi data pelatihan (80%) dan validasi (20%)
"""

# Acak dataframe
ratings = ratings.sample(frac=1, random_state=42)
ratings.reset_index(drop=True, inplace=True)
ratings

# Gabungkan kolom user dan book sebagai input fitur
x = ratings[['user', 'book']].values

# Normalisasi rating ke skala 0-1. Membuat variabel y untuk membuat rating dari hasil
y = ratings['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Bagi data menjadi train dan validation (80:20)
train_size = int(0.8 * len(ratings))
x_train, x_val = x[:train_size], x[train_size:]
y_train, y_val = y[:train_size], y[train_size:]

"""
Jika `min_rating` adalah 1 dan `max_rating` adalah 10, maka rating 1 menjadi 0, rating 10 menjadi 1, dan rating di antaranya diskalakan secara linier antara 0 dan 1. Jika rating 0 juga ada dan termasuk `min_rating`, maka 0 juga akan dipetakan ke 0"""

# Cek ukuran data hasil split
print(f"x_train shape: {x_train.shape}")
print(f"x_val shape: {x_val.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_val shape: {y_val.shape}")

"""# Model Development dengan Collaborative Filtering

Pada tahap ini, model menghitung skor kecocokan antara pengguna dan buku dengan teknik embedding.
1. Membuat embedding layer untuk user dan book agar sistem dapat belajar representasi vektor dari masing-masing entitas.
2. Menghitung dot product dari embedding user dan book untuk mendapatkan skor interaksi.
3. Menambahkan bias untuk setiap user dan book.
4. Menggunakan fungsi aktivasi sigmoid untuk memastikan skor output berada dalam skala [0, 1].

Di sini, dibuat class RecommenderNet dengan keras Model class. Kode class RecommenderNet ini terinspirasi dari tutorial dalam module [kelas Machine Learning Terapan Dicoding](https://www.dicoding.com/academies/319-machine-learning-terapan) dengan beberapa adaptasi yang disesuaikan dengan kasus Book Recommendation.
"""

class RecommenderNet(tf.keras.Model):

    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size

        # Embedding layer untuk user
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-5)
        )
        self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

        # Embedding layer untuk book
        self.book_embedding = layers.Embedding(
            num_books,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-5)
        )
        self.book_bias = layers.Embedding(num_books, 1) # layer embedding book bias

        # Tambahkan dropout
        self.user_dropout = layers.Dropout(0.2) # Coba dropout rate 0.2 atau 0.3
        self.book_dropout = layers.Dropout(0.2) # Gunakan dropout rate yang sama

    # memanggil layer
    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])

        # Tambahkan Dropout setelah embedding
        user_vector = self.user_dropout(user_vector)
        book_vector = self.book_dropout(book_vector)

        dot_user_book = tf.tensordot(user_vector, book_vector, 2)
        x = dot_user_book + user_bias + book_bias
        return tf.nn.sigmoid(x) # activation sigmoid

# Compile Model
model = RecommenderNet(num_users, num_books, embedding_size=50)

model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# EarlyStopping
early_stop = EarlyStopping(
    monitor='val_root_mean_squared_error',
    patience=5,  # hentikan jika tidak ada perbaikan dalam 5 epoch
    restore_best_weights=True
)

# Training Model
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[early_stop]
)

"""## Evaluation

Untuk melihat visualisasi proses training, mari kita plot metrik evaluasi
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""# Mendapatkan Rekomendasi Buku

Tahap ini bertujuan untuk memberikan rekomendasi buku personal kepada pengguna berdasarkan hasil prediksi model Collaborative Filtering. Langkah-langkah yang dilakukan:

- Memilih seorang pengguna secara acak.
- Menentukan daftar buku yang belum pernah diberi rating oleh pengguna tersebut.
- Melakukan encoding user dan buku untuk proses prediksi.
- Menggunakan model untuk menghitung prediksi skor rating.
- Menampilkan 10 buku dengan skor tertinggi sebagai rekomendasi personal.
- Menampilkan 5 buku yang sebelumnya mendapat rating tinggi dari user sebagai referensi preferensi.


"""

# Copy hasil rating setelah filtering dan preprocessing
rating_df = ratings.copy()

# Copy data dataframe buku asli (untuk info judul)
book_df = books.copy()

# Pilih satu user secara acak
user_id = rating_df['User-ID'].sample(1).iloc[0]
books_rating_by_user = rating_df[rating_df['User-ID'] == user_id]

# Ambil daftar ISBN buku yang belum pernah dibaca/dirating user
# Variabel books_not_rating diperoleh dengan menggunakan operator bitwise (~) pada variabel books_rating_by_user
books_not_rating = book_df[~book_df['ISBN'].isin(books_rating_by_user['ISBN'].values)]['ISBN']
books_not_rating = list(
    set(books_not_rating)
    .intersection(set(book_to_book_encoded.keys()))
)

# Encoding data buku belum dibaca/dirating dan user terkait
books_not_rating_encoded = [[book_to_book_encoded.get(x)] for x in books_not_rating]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(books_not_rating_encoded), books_not_rating_encoded)
)

# Prediksi rating untuk buku-buku yang belum dibaca user
ratings_pred = model.predict(user_book_array).flatten()

# Ambil 10 prediksi tertinggi
top_ratings_indices = ratings_pred.argsort()[-10:][::-1]
recommended_book_isbns = [
    book_encoded_to_book.get(books_not_rating_encoded[x][0]) for x in top_ratings_indices
]

print(f'\n Showing recommendations for user: {user_id}')
print('=' * 40)

# === Tampilkan Buku-Buku Favorit User ===
print('\n Books with high ratings from user:')
# Ambil buku yang dirating tinggi user, termasuk ratingnya
top_books_user_with_rating = (
    books_rating_by_user # Gunakan dataframe yang sudah berisi rating user
    .sort_values(by='Book-Rating', ascending=False)
    .head(5)
    [['ISBN', 'Book-Rating']] # Ambil ISBN dan Book-Rating
)

# Gabungkan dengan info buku (judul, penulis)
top_books_user_df = pd.merge(
    top_books_user_with_rating,
    book_df[['ISBN', 'Book-Title', 'Book-Author']],
    on='ISBN',
    how='left'
)

# Pilih kolom yang ingin ditampilkan dan urutkan kembali
top_books_user_df = top_books_user_df[['Book-Title', 'Book-Author', 'Book-Rating']]
top_books_user_df.index = range(1, len(top_books_user_df) + 1)
print(top_books_user_df.to_markdown()) # Tampilkan dalam format markdown table

# === Tampilkan Rekomendasi ===
print('\n Top 10 Book Recommendations:')
recommended_books_df = book_df[book_df['ISBN'].isin(recommended_book_isbns)].copy()

# Tambahkan kolom Predicted-Rating dari hasil prediksi
# ratings_pred adalah prediksi skala 0-1. Ambil prediksi yang sesuai dengan top 10 ISBN
predicted_ratings_for_top10 = ratings_pred[top_ratings_indices]
recommended_books_df['Predicted-Rating-Scale'] = predicted_ratings_for_top10

# Optional: Kembalikan prediksi ke skala rating asli (misalnya 1–10)
# Anda perlu menggunakan min_rating dan max_rating yang didapatkan dari data training 1-10
# Pastikan min_rating dan max_rating didefinisikan di sel sebelumnya
# min_rating = ratings['Book-Rating'].min() setelah filter 0 dan sparsity
# max_rating = ratings['Book-Rating'].max() setelah filter 0 dan sparsity
recommended_books_df['Predicted-Rating'] = recommended_books_df['Predicted-Rating-Scale'] * (max_rating - min_rating) + min_rating
recommended_books_df['Predicted-Rating'] = recommended_books_df['Predicted-Rating'].round(2) # Bulatkan 2 angka desimal

# Pilih kolom yang ingin ditampilkan dan urutkan berdasarkan prediksi
recommended_books_df = recommended_books_df[['Book-Title', 'Book-Author', 'Predicted-Rating']]
recommended_books_df = recommended_books_df.sort_values(by='Predicted-Rating', ascending=False).reset_index(drop=True)
recommended_books_df.index = range(1, len(recommended_books_df) + 1)

print(recommended_books_df.to_markdown()) # Tampilkan dalam format markdown table